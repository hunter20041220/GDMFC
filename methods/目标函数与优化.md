## goal function

$$
\min_{Z_i^{(v)},\, H_m^{(v)},\, \alpha^{(v)}} 
\quad 
\mathcal{L} 
= \sum_{v=1}^{V} (\alpha^{(v)})^\gamma 
\left(
    \left\| X^{(v)} - Z_1^{(v)} Z_2^{(v)} \dots Z_m^{(v)} H_m^{(v)} \right\|_F^2
    + \beta\, \mathrm{tr}\!\left(H_m^{(v)} L^{(v)} (H_m^{(v)})^T \right)
\right)
\\
- \lambda_1 \sum_{v \ne w}^V \mathrm{HSIC}(H_m^{(v)},\, H_m^{(w)})
+ \lambda_2 \sum_{v=1}^V \left\| H_m^{(v)} (H_m^{(v)})^T - I \right\|_F^2
\\[6pt]
\text{s.t.} \quad
H_m^{(v)} \ge 0,\quad 
\sum_{v=1}^V \alpha^{(v)} = 1,\quad
\alpha^{(v)} \ge 0.
$$



## Optimization

### 2.1 优化策略

提出一个高效的优化算法来求解目标函数。由于目标函数关于所有变量 $\{Z_i^{(v)}, H_m^{(v)}, \alpha^{(v)}\}$ 并非联合凸的,我们采用**交替最小化策略**,即每次固定其他变量,只优化一个变量块。

### 2.2 预训练阶段

借鉴深度矩阵分解的思想,我们首先对每个视图 $v$ 进行逐层预训练,以获得良好的初始化:

**第一层**: 对每个视图 $v$,求解
$$\min_{Z_1^{(v)}, H_1^{(v)}} \left\| X^{(v)} - Z_1^{(v)} H_1^{(v)} \right\|_F^2, \quad \text{s.t. } H_1^{(v)} \geq 0$$

**第 $i$ 层** ($i = 2, \ldots, m-1$): 固定前 $i-1$ 层,求解
$$\min_{Z_i^{(v)}, H_i^{(v)}} \left\| H_{i-1}^{(v)} - Z_i^{(v)} H_i^{(v)} \right\|_F^2, \quad \text{s.t. } H_i^{(v)} \geq 0$$

这种逐层预训练为后续的全局优化提供了良好的初始点,避免陷入较差的局部最优。

### 2.3 微调阶段 

在预训练后,我们对所有层进行联合优化。为了简化推导,我们定义辅助矩阵:

$$\Phi_i^{(v)} = Z_1^{(v)} Z_2^{(v)} \cdots Z_{i-1}^{(v)}, \quad \Psi_i^{(v)} = Z_{i+1}^{(v)} \cdots Z_m^{(v)} H_m^{(v)}$$

则重构项可以写为:
$$X^{(v)} \approx \Phi_i^{(v)} Z_i^{(v)} \Psi_i^{(v)}$$

#### 2.3.1 更新 $Z_i^{(v)}$ (中间层基矩阵)

固定 $H_m^{(v)}$ 和其他 $Z_j^{(v)}$ ($j \neq i$),优化 $Z_i^{(v)}$。对于中间层 $i < m$,目标函数简化为:

$$\mathcal{L}_{Z_i} = \left\| X^{(v)} - \Phi_i^{(v)} Z_i^{(v)} \Psi_i^{(v)} \right\|_F^2$$

令 $\frac{\partial \mathcal{L}_{Z_i}}{\partial Z_i^{(v)}} = 0$,得到:

$$\frac{\partial}{\partial Z_i^{(v)}} \text{tr}\left[ (X^{(v)} - \Phi_i^{(v)} Z_i^{(v)} \Psi_i^{(v)})^T (X^{(v)} - \Phi_i^{(v)} Z_i^{(v)} \Psi_i^{(v)}) \right] = 0$$

展开并求导:
$$-2 (\Phi_i^{(v)})^T X^{(v)} (\Psi_i^{(v)})^T + 2 (\Phi_i^{(v)})^T \Phi_i^{(v)} Z_i^{(v)} \Psi_i^{(v)} (\Psi_i^{(v)})^T = 0$$

因此,**更新规则**为:
$$\boxed{Z_i^{(v)} = \left[ (\Phi_i^{(v)})^T \Phi_i^{(v)} \right]^{-1} (\Phi_i^{(v)})^T X^{(v)} \left[ (\Psi_i^{(v)})^T \right]^\dagger}$$

或使用伪逆形式:
$$\boxed{Z_i^{(v)} = (\Phi_i^{(v)})^\dagger X^{(v)} (\Psi_i^{(v)})^\dagger}$$

其中 $\dagger$ 表示 Moore-Penrose 伪逆。

#### 2.3.2 更新 $H_m^{(v)}$ (顶层表示矩阵)

这是最关键也是最复杂的一步。固定所有 $Z_i^{(v)}$ 和 $\alpha^{(v)}$,优化 $H_m^{(v)}$。目标函数为:

$$\mathcal{L}_{H_m} = \sum_{v=1}^{V} (\alpha^{(v)})^\gamma \left( \left\| X^{(v)} - Z_1^{(v)} \cdots Z_m^{(v)} H_m^{(v)} \right\|_F^2 + \beta\, \text{tr}\!\left(H_m^{(v)} L^{(v)} (H_m^{(v)})^T \right) \right) \\ - \lambda_1 \sum_{v \ne w}^V \text{HSIC}(H_m^{(v)},\, H_m^{(w)}) + \lambda_2 \sum_{v=1}^V \left\| H_m^{(v)} (H_m^{(v)})^T - I \right\|_F^2$$

为了处理这个复杂的目标,我们逐项分析:

##### (A) 重构误差项的梯度

定义 $\Phi_m^{(v)} = Z_1^{(v)} Z_2^{(v)} \cdots Z_m^{(v)}$,则:

$$\frac{\partial}{\partial H_m^{(v)}} \left\| X^{(v)} - \Phi_m^{(v)} H_m^{(v)} \right\|_F^2 = 2 (\Phi_m^{(v)})^T (\Phi_m^{(v)} H_m^{(v)} - X^{(v)})$$

$$= 2 (\Phi_m^{(v)})^T \Phi_m^{(v)} H_m^{(v)} - 2 (\Phi_m^{(v)})^T X^{(v)}$$

##### (B) 图正则化项的梯度

回顾 $L^{(v)} = D^{(v)} - W^{(v)}$,其中 $W^{(v)}$ 是权重矩阵,$D^{(v)}$ 是度矩阵:

$$\frac{\partial}{\partial H_m^{(v)}} \text{tr}\!\left(H_m^{(v)} L^{(v)} (H_m^{(v)})^T \right) = 2 H_m^{(v)} L^{(v)} = 2 H_m^{(v)} (D^{(v)} - W^{(v)})$$

##### (C) HSIC 多样性项的梯度

根据 DiMSC 论文,使用内积核 $K^{(v)} = (H_m^{(v)})^T H_m^{(v)}$:

$$\text{HSIC}(H_m^{(v)}, H_m^{(w)}) = \text{tr}(H K^{(w)} H (H_m^{(v)})^T H_m^{(v)})$$

其中 $H = I - \frac{1}{n}\mathbf{1}\mathbf{1}^T$ 是中心化矩阵。

$$\frac{\partial}{\partial H_m^{(v)}} \sum_{w \neq v} \text{HSIC}(H_m^{(v)}, H_m^{(w)}) = 2 H_m^{(v)} \sum_{w \neq v} H K^{(w)} H$$

定义辅助矩阵:
$$\boxed{K_{-v} = \sum_{w \neq v} H K^{(w)} H = \sum_{w \neq v} H (H_m^{(w)})^T H_m^{(w)} H}$$

##### (D) 正交约束项的梯度

$$\frac{\partial}{\partial H_m^{(v)}} \left\| H_m^{(v)} (H_m^{(v)})^T - I \right\|_F^2$$

$$= \frac{\partial}{\partial H_m^{(v)}} \text{tr}\left[ (H_m^{(v)} (H_m^{(v)})^T - I)(H_m^{(v)} (H_m^{(v)})^T - I)^T \right]$$

$$= \frac{\partial}{\partial H_m^{(v)}} \left[ \text{tr}(H_m^{(v)} (H_m^{(v)})^T H_m^{(v)} (H_m^{(v)})^T) - 2\text{tr}(H_m^{(v)} (H_m^{(v)})^T) + n \right]$$

$$= 4 H_m^{(v)} (H_m^{(v)})^T H_m^{(v)} - 4 H_m^{(v)}$$

##### (E) 完整梯度

综合以上各项,关于 $H_m^{(v)}$ 的完整梯度为:

$$\nabla_{H_m^{(v)}} \mathcal{L} = 2(\alpha^{(v)})^\gamma \left[ (\Phi_m^{(v)})^T \Phi_m^{(v)} H_m^{(v)} - (\Phi_m^{(v)})^T X^{(v)} + \beta H_m^{(v)} (D^{(v)} - W^{(v)}) \right]$$

$$- 2\lambda_1 H_m^{(v)} K_{-v} + 4\lambda_2 (H_m^{(v)} (H_m^{(v)})^T H_m^{(v)} - H_m^{(v)})$$

##### (F) 乘法更新规则

为了满足非负约束 $H_m^{(v)} \geq 0$,我们采用乘法更新规则。将梯度分解为正部和负部:

$$\nabla^+ = 2(\alpha^{(v)})^\gamma \left[ (\Phi_m^{(v)})^T \Phi_m^{(v)} H_m^{(v)} + \beta H_m^{(v)} D^{(v)} \right] + 4\lambda_2 H_m^{(v)} (H_m^{(v)})^T H_m^{(v)}$$

$$\nabla^- = 2(\alpha^{(v)})^\gamma \left[ (\Phi_m^{(v)})^T X^{(v)} + \beta H_m^{(v)} W^{(v)} \right] + 2\lambda_1 H_m^{(v)} K_{-v} + 4\lambda_2 H_m^{(v)}$$

**更新规则**:
$$\boxed{H_m^{(v)} \leftarrow H_m^{(v)} \odot \sqrt{\frac{\nabla^-}{\nabla^+ + \epsilon}}}$$

或展开形式:
$$\boxed{H_m^{(v)} \leftarrow H_m^{(v)} \odot \sqrt{\frac{(\alpha^{(v)})^\gamma [(\Phi_m^{(v)})^T X^{(v)} + \beta H_m^{(v)} W^{(v)}] + \lambda_1 H_m^{(v)} K_{-v} + 2\lambda_2 H_m^{(v)}}{(\alpha^{(v)})^\gamma [(\Phi_m^{(v)})^T \Phi_m^{(v)} H_m^{(v)} + \beta H_m^{(v)} D^{(v)}] + 2\lambda_2 H_m^{(v)} (H_m^{(v)})^T H_m^{(v)} + \epsilon}}}$$

其中 $\odot$ 表示逐元素乘法,$\epsilon$ 是一个小的正数(如 $10^{-10}$)以避免除零。

#### 2.3.3 更新 $\alpha^{(v)}$ (视图权重)

固定 $Z_i^{(v)}$ 和 $H_m^{(v)}$,优化 $\alpha^{(v)}$。定义第 $v$ 个视图的总损失:

$$\mathcal{R}^{(v)} = \left\| X^{(v)} - Z_1^{(v)} \cdots Z_m^{(v)} H_m^{(v)} \right\|_F^2 + \beta\, \text{tr}\!\left(H_m^{(v)} L^{(v)} (H_m^{(v)})^T \right)$$

则关于 $\alpha^{(v)}$ 的子问题为:

$$\min_{\alpha^{(v)}} \sum_{v=1}^{V} (\alpha^{(v)})^\gamma \mathcal{R}^{(v)}, \quad \text{s.t. } \sum_{v=1}^{V} \alpha^{(v)} = 1, \alpha^{(v)} \geq 0$$

构造拉格朗日函数:
$$\mathcal{L}_\alpha = \sum_{v=1}^{V} (\alpha^{(v)})^\gamma \mathcal{R}^{(v)} - \mu \left( \sum_{v=1}^{V} \alpha^{(v)} - 1 \right)$$

令 $\frac{\partial \mathcal{L}_\alpha}{\partial \alpha^{(v)}} = 0$:
$$\gamma (\alpha^{(v)})^{\gamma-1} \mathcal{R}^{(v)} - \mu = 0$$

$$\Rightarrow \alpha^{(v)} = \left( \frac{\mu}{\gamma \mathcal{R}^{(v)}} \right)^{\frac{1}{\gamma-1}}$$

代入约束 $\sum_{v=1}^{V} \alpha^{(v)} = 1$:

$$\sum_{v=1}^{V} \left( \frac{\mu}{\gamma \mathcal{R}^{(v)}} \right)^{\frac{1}{\gamma-1}} = 1$$

$$\Rightarrow \mu^{\frac{1}{\gamma-1}} = \left( \sum_{v=1}^{V} (\gamma \mathcal{R}^{(v)})^{-\frac{1}{\gamma-1}} \right)^{-1}$$

因此,**更新规则**为:
$$\boxed{\alpha^{(v)} = \frac{(\mathcal{R}^{(v)})^{-\frac{1}{\gamma-1}}}{\sum_{w=1}^{V} (\mathcal{R}^{(w)})^{-\frac{1}{\gamma-1}}}}$$

或等价地:
$$\boxed{\alpha^{(v)} = \frac{(\mathcal{R}^{(v)})^{\frac{1}{1-\gamma}}}{\sum_{w=1}^{V} (\mathcal{R}^{(w)})^{\frac{1}{1-\gamma}}}}$$

**参数 $\gamma$ 的作用**:

- 当 $\gamma \to 1^+$ 时,$\alpha^{(v)}$ 趋向于将所有权重分配给损失最小的视图(硬选择)
- 当 $\gamma \to \infty$ 时,$\alpha^{(v)} \to 1/V$(所有视图等权重)
- 通常设置 $\gamma \in (1, 2]$ 以平衡多视图